{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook with Code to Clean the Data Scraped off the Census of India [Website](https://censusindia.gov.in/census.website/data/census-tables)\n",
        "\n",
        "By way of \"cleaning\" this notebook does the following:\n",
        "\n",
        "1. Fix column header structure of each data file according to the [Metadata file](https://github.com/SocratusCollective/gender-polyconflict/blob/main/Metadata_2011.xlsx).\n",
        "1. Strip leading spaces and replace \"-\" with \" to \" in Age Group columns, for e.g., replace \" 5-12\" by \"5 to 12\", to prevent spreadsheet software from misinterpreting the age range \"5-12\" as the date, the 5th of December, for example.\n",
        "1. Standardise hyphenation in the Area Name column,\n",
        "1. Remove rows where any primary key columns are empty,\n",
        "1. Perform meta-data checks, which consist of:\n",
        "```\n",
        "a) Checking for primary key uniqueness\n",
        "b) Data type match\n",
        "c) Checking if columns expected to have 1:1 mappings do\n",
        "d) Checking levels of categorical variables (if any)\n",
        "```\n",
        "1. In special cases, the following processing steps will additionally be run _before_ meta-data checks:\n",
        "```\n",
        "a) If Census file type is \"H-01\" or \"HH-01-Total\", delete all rows where\n",
        "\"Tehsil Code\" <> \"00000\" or \"Town Code\" <> \"000000\", or,\n",
        "b) If Census file type is \"HL-14-Total\" or \"HL-14-SC-ST\", delete all rows where\n",
        "\"Tehsil Code\" <> \"00000\" or \"Town Code/Village code\" <> \"000000\" or\n",
        "\"Ward No <> \"0000\", and ensure values across proportion columns sum up\n",
        "to 100 in each row.\n",
        "c) If \"F-04\", \"F-08\" or \"F-12\", remove leading whitespaces from \"Economic Activity\" column.\n",
        "```\n",
        "\n",
        "1. Collate similar datasets (e.g., for the \"B-01\" type of files, there is one file for each state with caste granularity \"Total\", \"SC\" and \"ST\"; so the task is to obtain one collated file for each caste category, \"Total\", \"SC\" and \"ST\") and output to [this](https://drive.google.com/drive/folders/1N7hefMlzIodrufTz7xHGM1boKv0Wvozq) folder.\n",
        "  - Note, however, that if the collated dataset size exceeds Google Sheets's limit of 10 million cells per file, each dataset will be stored as a separate file."
      ],
      "metadata": {
        "id": "mvAv7Ms9ZL32"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpnnq-DrZAad"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet gspread google-auth google-api-python-client \\\n",
        "  pandas==2.2.2 google-auth==2.27.0 openpyxl xlrd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "from googleapiclient.discovery import build\n",
        "import gspread\n",
        "import pandas as pd\n",
        "from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload\n",
        "import io\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "TbQusEpxbpVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Util Function: Get ID of a folder (a unique identifier for the folder in Google Drive) given its path"
      ],
      "metadata": {
        "id": "wL59iUB0_6Rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_folder_id(path):\n",
        "    \"\"\"\n",
        "    Retrieves the Google Drive folder ID for a given path.\n",
        "\n",
        "    Args:\n",
        "    -----\n",
        "        path (str): Path in Google Drive (e.g., \"RootFolder/SubFolder/SubSubFolder\").\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "        str: Folder ID if found, else None.\n",
        "    \"\"\"\n",
        "    path = path.strip('/')  # Normalize path\n",
        "    folder_id = 'root'  # Start from the root folder\n",
        "\n",
        "    for folder_name in path.split('/'):\n",
        "        query = (f\"'{folder_id}' in parents and mimeType='application/\"\n",
        "                 f\"vnd.google-apps.folder' and name='{folder_name}'\")\n",
        "        results = drive_service.files().list(q=query, fields=\"files(id)\").execute()\n",
        "        folders = results.get('files', [])\n",
        "\n",
        "        if not folders:\n",
        "            return None  # Return None if the folder is not found\n",
        "\n",
        "        folder_id = folders[0]['id']  # Assume unique folder names\n",
        "\n",
        "    return folder_id"
      ],
      "metadata": {
        "id": "bIl7eTNwc5p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to cache Metadata for a given dataset from the Metadata Google Sheet"
      ],
      "metadata": {
        "id": "9MOZV6JJ91m6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cache_metadata_from_sheet(sheet, dataset_name, start_cell):\n",
        "    \"\"\"\n",
        "    Extract metadata for a given dataset name from a Google Sheet.\n",
        "\n",
        "    Parameters:\n",
        "    - sheet (gspread.Spreadsheet): The Google Sheet object.\n",
        "    - dataset_name (str): Dataset name, which is also the name of the tab\n",
        "                          containing the dataset's metadata.\n",
        "    - start_cell (str): Starting cell of the metadata (e.g., 'F1').\n",
        "\n",
        "    Returns:\n",
        "    - dict: Metadata dictionary with column names, primary keys, types, and levels.\n",
        "    \"\"\"\n",
        "    if dataset_name in metadata_cache:\n",
        "        print(f\"Metadata for {dataset_name} already cached.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Access the specific tab\n",
        "        tab = sheet.worksheet(dataset_name)\n",
        "\n",
        "        # Fetch all tab content\n",
        "        all_values = tab.get_all_values()\n",
        "\n",
        "        # Calculate row and column indices for start_cell;\n",
        "        # e.g., D3 corresponds to row 3 and column 4\n",
        "        start_row, start_col = gspread.utils.a1_to_rowcol(start_cell)\n",
        "\n",
        "        # Filter all tab content to get content from the given start_cell\n",
        "        filtered_values = [row[start_col - 1 :] \\\n",
        "                           for row in all_values[start_row - 1 :] if any(row)]\n",
        "\n",
        "        # Determine the last column and row dynamically\n",
        "        last_col_index = max(len(row) for row in filtered_values)\n",
        "        metadata = [row[:last_col_index] for row in filtered_values]\n",
        "\n",
        "        # Convert metadata into a dictionary\n",
        "        metadata_dict = {}\n",
        "        for row in metadata[1:]:  # Skip header row\n",
        "            column_name = row[1].strip() if len(row) > 1 else None\n",
        "            if column_name:  # Only process rows with a valid column name\n",
        "                primary_key = row[2] == \"Y\" if len(row) > 2 else False\n",
        "                map_1_1 = row[3] if len(row) > 3 else None\n",
        "                data_type = row[4] if len(row) > 4 else None\n",
        "                levels = [level for level in row[5:] if level]\n",
        "\n",
        "                if data_type == \"constant string\":\n",
        "                  assert len(levels) == 1, (f\"Encountered Constant String\"\n",
        "                                            f\" variable {column_name} with\"\n",
        "                                            f\"multiple levels {levels}\")\n",
        "\n",
        "                metadata_dict[column_name] = {\n",
        "                    \"Primary Key\": primary_key,\n",
        "                    \"Map 1:1\": map_1_1,\n",
        "                    \"Type\": data_type,\n",
        "                    \"Levels\": levels,\n",
        "                }\n",
        "\n",
        "        # Cache the metadata for this dataset name\n",
        "        metadata_cache[dataset_name] = metadata_dict\n",
        "        print(f\"Metadata for {dataset_name} cached successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving metadata for dataset name {dataset_name}: {e}\")"
      ],
      "metadata": {
        "id": "YjE7uHzB-PA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to get IDs and names of relevant files\n",
        "This function fetches the IDs and full names of all files in a given a google drive `folder_id` and beggining with the `input_prefix` (e.g., \"B-01\"). If `caste` is specified, relevant files will be deemed to be those corresponding to the specified caste (i.e., files which end with `f\"_{caste.lower()}.xlsx\"` or `f\"_{caste.lower()}.xls\"`).\n",
        "\n",
        "Notes:\n",
        "- File ID is a unique identifier for a given file in Google Drive\n",
        "- Function excludes those files which begin with `f\"{input_prefix}_India\"`"
      ],
      "metadata": {
        "id": "skAzuLXWAftF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file_ids_and_names(folder_id, input_prefix, caste=None):\n",
        "    \"\"\"\n",
        "    Retrieves ID and names of files starting with the specified prefix, excluding\n",
        "    files starting with f\"{input_prefix}_India\". If caste is specified, filters\n",
        "    further to files ending with f\"_{caste.lower()}.xlsx\" or f\"_{caste.lower()}.xls\".\n",
        "\n",
        "    Parameters:\n",
        "    - folder_id (str): ID of Google Drive folder containing the files.\n",
        "    - input_prefix (str): Prefix of file names to process.\n",
        "    - caste (str, optional): Specific caste filter for file names.\n",
        "\n",
        "    Returns:\n",
        "    - List[dict]: A list of ID/name dictionaries for relevant files.\n",
        "    \"\"\"\n",
        "    query = f\"'{folder_id}' in parents\"\n",
        "    files = []\n",
        "    page_token = None\n",
        "\n",
        "    while True:\n",
        "        response = drive_service.files().list(\n",
        "            q=query,\n",
        "            spaces='drive',\n",
        "            fields=\"nextPageToken, files(id, name)\",\n",
        "            pageToken=page_token\n",
        "        ).execute()\n",
        "\n",
        "        files.extend(response.get('files', []))\n",
        "        page_token = response.get('nextPageToken', None)\n",
        "\n",
        "        if not page_token:\n",
        "            break\n",
        "\n",
        "    # Filter files by prefix and exclude f\"{input_prefix}_India\"\n",
        "    relevant_files = [\n",
        "        file for file in files\n",
        "        if file['name'].startswith(input_prefix) and not \\\n",
        "        file['name'].startswith(f\"{input_prefix}_India\")\n",
        "    ]\n",
        "\n",
        "    # Apply caste filter if specified\n",
        "    if caste:\n",
        "        caste_lower = caste.lower()\n",
        "        relevant_files = [\n",
        "            file for file in relevant_files if file['name'].lower().endswith(\n",
        "                (f\"_{caste_lower}.xlsx\", f\"_{caste_lower}.xls\"))\n",
        "        ]\n",
        "\n",
        "        print((f\"{len(relevant_files)} files found starting with the prefix \"\n",
        "              f\"'{input_prefix}' (excluding '{input_prefix}_India') \"\n",
        "              f\"and matching caste '{caste}'.\"))\n",
        "    else:\n",
        "\n",
        "        print((f\"{len(relevant_files)} files found starting with the prefix \"\n",
        "              f\"'{input_prefix}' (excluding '{input_prefix}_India').\"))\n",
        "    return relevant_files"
      ],
      "metadata": {
        "id": "8NOztVZ8eAO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions to download a given single Census data file and return a processed `pandas` dataframe containing the Census data:"
      ],
      "metadata": {
        "id": "8A0MI4z2YmsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalise_dataframe(df, file_name):\n",
        "    \"\"\"\n",
        "    Normalises values in predefined sets of columns so that they sum to 100.\n",
        "    \"\"\"\n",
        "    df_normalised = df.copy()\n",
        "\n",
        "    try:\n",
        "\n",
        "      # Normalizing set 1\n",
        "      columns_set_1 = [\n",
        "          \"Census House Condition - Usetype: Residence: Good\",\n",
        "          \"Census House Condition - Usetype: Residence: Livable\",\n",
        "          \"Census House Condition - Usetype: Residence: Dilapidated\",\n",
        "          \"Census House Condition - Usetype: Residence-cum-other use: Good\",\n",
        "          \"Census House Condition - Usetype: Residence-cum-other use: Livable\",\n",
        "          \"Census House Condition - Usetype: Residence-cum-other use: Dilapidated\"\n",
        "          ]\n",
        "      row_sums_1 = df[columns_set_1].sum(axis=1, skipna=True)\n",
        "      row_sums_1[row_sums_1 == 0] = 1  # Avoid division by zero\n",
        "      df_normalised[columns_set_1] = \\\n",
        "       (df[columns_set_1].div(row_sums_1, axis=0)) * 100\n",
        "\n",
        "      # Normalizing set 2\n",
        "      columns_set_2 = [\"Material of Roof: Grass/ Thatch/ Bamboo/ Wood/Mud etc.\",\n",
        "                      \"Material of Roof: Plastic/ Polythene\",\n",
        "                      \"Material of Roof: Hand made Tiles\",\n",
        "                      \"Material of Roof: Machine made Tiles\",\n",
        "                      \"Material of Roof: Burnt Brick\",\n",
        "                      \"Material of Roof: Stone/ Slate\",\n",
        "                      \"Material of Roof: G.I./Metal/ Asbestos sheets\",\n",
        "                      \"Material of Roof: Concrete\",\n",
        "                      \"Material of Roof: Any other material\"]\n",
        "      row_sums_2 = df[columns_set_2].sum(axis=1, skipna=True)\n",
        "      row_sums_2[row_sums_2 == 0] = 1  # Avoid division by zero\n",
        "      df_normalised[columns_set_2] = \\\n",
        "       (df[columns_set_2].div(row_sums_2, axis=0)) * 100\n",
        "\n",
        "      # Normalizing set 3\n",
        "      columns_set_3 = [\"Material of Wall: Grass/ Thatch/ Bamboo etc.\",\n",
        "                      \"Material of Wall: Plastic/ Polythene\",\n",
        "                      \"Material of Wall: Mud/Unburnt brick\",\n",
        "                      \"Material of Wall: Wood\",\n",
        "                      \"Material of Wall: Stone not packed with mortar\",\n",
        "                      \"Material of Wall: Stone packed with mortar\",\n",
        "                      \"Material of Wall: G.I./ Metal/ Asbestos sheets\",\n",
        "                      \"Material of Wall: Burnt brick\",\n",
        "                      \"Material of Wall: Concrete\",\n",
        "                      \"Material of Wall: Any other material\"]\n",
        "      row_sums_3 = df[columns_set_3].sum(axis=1, skipna=True)\n",
        "      row_sums_3[row_sums_3 == 0] = 1  # Avoid division by zero\n",
        "      df_normalised[columns_set_3] = \\\n",
        "       (df[columns_set_3].div(row_sums_3, axis=0)) * 100\n",
        "\n",
        "      # Normalizing set 4\n",
        "      columns_set_4 = [\"Material of Floor: Mud\",\n",
        "                      \"Material of Floor: Wood/ Bamboo\",\n",
        "                      \"Material of Floor: Burnt Brick\",\n",
        "                      \"Material of Floor: Stone\",\n",
        "                      \"Material of Floor: Cement\",\n",
        "                      \"Material of Floor: Mosaic/ Floor tiles\",\n",
        "                      \"Material of Floor: Any other material\"]\n",
        "      row_sums_4 = df[columns_set_4].sum(axis=1, skipna=True)\n",
        "      row_sums_4[row_sums_4 == 0] = 1  # Avoid division by zero\n",
        "      df_normalised[columns_set_4] = \\\n",
        "       (df[columns_set_4].div(row_sums_4, axis=0)) * 100\n",
        "\n",
        "      # Normalizing set 5\n",
        "      columns_set_5 = [\"Number of Dwelling Rooms: No exclusive room\",\n",
        "                      \"Number of Dwelling Rooms: One room\",\n",
        "                      \"Number of Dwelling Rooms: Two rooms\",\n",
        "                      \"Number of Dwelling Rooms: Three rooms\",\n",
        "                      \"Number of Dwelling Rooms: Four rooms\",\n",
        "                      \"Number of Dwelling Rooms: Five rooms\",\n",
        "                      \"Number of Dwelling Rooms: Six rooms and above\"]\n",
        "      row_sums_5 = df[columns_set_5].sum(axis=1, skipna=True)\n",
        "      row_sums_5[row_sums_5 == 0] = 1  # Avoid division by zero\n",
        "      df_normalised[columns_set_5] = \\\n",
        "       (df[columns_set_5].div(row_sums_5, axis=0)) * 100\n",
        "\n",
        "      # Normalizing set 6\n",
        "      columns_set_6 = [\"Household size: 1\", \"Household size: 2\",\n",
        "                      \"Household size: 3\", \"Household size: 4\",\n",
        "                      \"Household size: 5\", \"Household size: 6-8\",\n",
        "                      \"Household size: 9+\"]\n",
        "      row_sums_6 = df[columns_set_6].sum(axis=1, skipna=True)\n",
        "      row_sums_6[row_sums_6 == 0] = 1  # Avoid division by zero\n",
        "      df_normalised[columns_set_6] = \\\n",
        "       (df[columns_set_6].div(row_sums_6, axis=0)) * 100\n",
        "\n",
        "      # Normalizing set 7\n",
        "      columns_set_7 = [\"Ownership status: Owned\", \"Ownership status: Rented\",\n",
        "                      \"Ownership status: Any others\"]\n",
        "      row_sums_7 = df[columns_set_7].sum(axis=1, skipna=True)\n",
        "      row_sums_7[row_sums_7 == 0] = 1  # Avoid division by zero\n",
        "      df_normalised[columns_set_7] = \\\n",
        "       (df[columns_set_7].div(row_sums_7, axis=0)) * 100\n",
        "\n",
        "      # Normalizing set 8\n",
        "      columns_set_8 = [\"Married couple: None\", \"Married couple: 1\",\n",
        "                      \"Married couple: 2\", \"Married couple: 3\",\n",
        "                      \"Married couple: 4\", \"Married couple: 5+\"]\n",
        "      row_sums_8 = df[columns_set_8].sum(axis=1, skipna=True)\n",
        "      row_sums_8[row_sums_8 == 0] = 1  # Avoid division by zero\n",
        "      df_normalised[columns_set_8] = \\\n",
        "       (df[columns_set_8].div(row_sums_8, axis=0)) * 100\n",
        "\n",
        "      # Normalizing set 9\n",
        "      columns_set_9 = [\n",
        "          \"Main Source of Drinking Water: Tapwater from treated source\",\n",
        "          \"Main Source of Drinking Water: Tapwater from un-treated source\",\n",
        "          \"Main Source of Drinking Water: Covered well\",\n",
        "          \"Main Source of Drinking Water: Un-covered well\",\n",
        "          \"Main Source of Drinking Water: Handpump\",\n",
        "          \"Main Source of Drinking Water: Tubewell/Borehole\",\n",
        "          \"Main Source of Drinking Water: Spring\",\n",
        "          \"Main Source of Drinking Water: River/Canal\",\n",
        "          \"Main Source of Drinking Water: Tank/Pond/Lake\",\n",
        "          \"Main Source of Drinking Water: Other sources\"]\n",
        "      row_sums_9 = df[columns_set_9].sum(axis=1, skipna=True)\n",
        "      row_sums_9[row_sums_9 == 0] = 1  # Avoid division by zero\n",
        "      df_normalised[columns_set_9] = \\\n",
        "       (df[columns_set_9].div(row_sums_9, axis=0)) * 100\n",
        "\n",
        "      # Normalizing set 10\n",
        "      columns_set_10 = [\"Location of drinking water source: Within premises\",\n",
        "                        \"Location of drinking water source: Near premises\",\n",
        "                        \"Location of drinking water source: Away\"]\n",
        "      row_sums_10 = df[columns_set_10].sum(axis=1, skipna=True)\n",
        "      row_sums_10[row_sums_10 == 0] = 1  # Avoid division by zero\n",
        "      df_normalised[columns_set_10] = \\\n",
        "       (df[columns_set_10].div(row_sums_10, axis=0)) * 100\n",
        "\n",
        "      # Normalizing set 11\n",
        "      columns_set_11 = [\"Main Source of lighting: Electricity\",\n",
        "                        \"Main Source of lighting: Kerosene\",\n",
        "                        \"Main Source of lighting: Solar energy\",\n",
        "                        \"Main Source of lighting: Other oil\",\n",
        "                        \"Main Source of lighting: Any other\",\n",
        "                        \"Main Source of lighting: No lighting\"]\n",
        "      row_sums_11 = df[columns_set_11].sum(axis=1, skipna=True)\n",
        "      row_sums_11[row_sums_11 == 0] = 1  # Avoid division by zero\n",
        "      df_normalised[columns_set_11] = \\\n",
        "       (df[columns_set_11].div(row_sums_11, axis=0)) * 100\n",
        "\n",
        "      # Normalizing set 12\n",
        "      columns_set_12 = [\n",
        "        \"Latrine in-premises: Flush/pour flush latrine connected to: Piped sewer system\",\n",
        "        \"Latrine in-premises: Flush/pour flush latrine connected to: Septic tank\",\n",
        "        \"Latrine in-premises: Flush/pour flush latrine connected to: Other system\",\n",
        "        \"Latrine in-premises: Pit latrine: With slab/ventilated improved pit\",\n",
        "        \"Latrine in-premises: Pit latrine: Without slab/ open pit\",\n",
        "        \"Latrine in-premises: Night soil disposed into open drain\",\n",
        "        \"Latrine in-premises: Service Latrine: Night soil removed by human\",\n",
        "        \"Latrine in-premises: Service Latrine: Night soil serviced by animal\",\n",
        "        \"No latrine in-premises: Public latrine\", \"No latrine in-premises: Open\"]\n",
        "      row_sums_12 = df[columns_set_12].sum(axis=1, skipna=True)\n",
        "      row_sums_12[row_sums_12 == 0] = 1  # Avoid division by zero\n",
        "      df_normalised[columns_set_12] = \\\n",
        "       (df[columns_set_12].div(row_sums_12, axis=0)) * 100\n",
        "\n",
        "      # Normalizing set 13\n",
        "      columns_set_13 = [\"Bathing facility in premises: Yes: Bathroom\",\n",
        "                        \"Bathing facility in premises: Yes: Enclosure without roof\",\n",
        "                        \"Bathing facility in premises: No\"]\n",
        "      row_sums_13 = df[columns_set_13].sum(axis=1, skipna=True)\n",
        "      row_sums_13[row_sums_13 == 0] = 1  # Avoid division by zero\n",
        "      df_normalised[columns_set_13] = \\\n",
        "       (df[columns_set_13].div(row_sums_13, axis=0)) * 100\n",
        "\n",
        "      # Normalizing set 14\n",
        "      columns_set_14 = [\"Waste water outlet connected to: Closed drainage\",\n",
        "                        \"Waste water outlet connected to: Open drainage\",\n",
        "                        \"Waste water outlet connected to: No drainage\"]\n",
        "      row_sums_14 = df[columns_set_14].sum(axis=1, skipna=True)\n",
        "      row_sums_14[row_sums_14 == 0] = 1  # Avoid division by zero\n",
        "      df_normalised[columns_set_14] = \\\n",
        "       (df[columns_set_14].div(row_sums_14, axis=0)) * 100\n",
        "\n",
        "      # Normalizing set 15\n",
        "      columns_set_15 = [\"Type of Fuel used for Cooking: Fire-wood\",\n",
        "                        \"Type of Fuel used for Cooking: Crop residue\",\n",
        "                        \"Type of Fuel used for Cooking: Cowdung cake\",\n",
        "                        \"Type of Fuel used for Cooking: Coal,Lignite,Charcoal\",\n",
        "                        \"Type of Fuel used for Cooking: Kerosene\",\n",
        "                        \"Type of Fuel used for Cooking: LPG/PNG\",\n",
        "                        \"Type of Fuel used for Cooking: Electricity\",\n",
        "                        \"Type of Fuel used for Cooking: Biogas\",\n",
        "                        \"Type of Fuel used for Cooking: Any other\",\n",
        "                        \"Type of Fuel used for Cooking: No cooking\"]\n",
        "      row_sums_15 = df[columns_set_15].sum(axis=1, skipna=True)\n",
        "      row_sums_15[row_sums_15 == 0] = 1  # Avoid division by zero\n",
        "      df_normalised[columns_set_15] = \\\n",
        "       (df[columns_set_15].div(row_sums_15, axis=0)) * 100\n",
        "\n",
        "      # Normalizing set 16\n",
        "      columns_set_16 = [\"Cooking inside house: Has Kitchen\",\n",
        "                        \"Cooking inside house: Does not have kitchen\",\n",
        "                        \"Cooking outside house: Has Kitchen\",\n",
        "                        \"Cooking outside house: Does not have kitchen\",\n",
        "                        \"No Cooking\"]\n",
        "      row_sums_16 = df[columns_set_16].sum(axis=1, skipna=True)\n",
        "      row_sums_16[row_sums_16 == 0] = 1  # Avoid division by zero\n",
        "      df_normalised[columns_set_16] = \\\n",
        "       (df[columns_set_16].div(row_sums_16, axis=0)) * 100\n",
        "\n",
        "      # Normalizing set 17\n",
        "      columns_set_17 = [\"Type of Census Houses: Permanent\",\n",
        "                        \"Type of Census Houses: Semi-Permanent\",\n",
        "                        \"Type of Census Houses: Total Temporary\",\n",
        "                        \"Type of Census Houses: Temporary - Serviceable\",\n",
        "                        \"Type of Census Houses: Temporary - Non-Serviceable\",\n",
        "                        \"Type of Census Houses: Unclassifiable\"]\n",
        "      row_sums_17 = df[columns_set_17].sum(axis=1, skipna=True)\n",
        "      row_sums_17[row_sums_17 == 0] = 1  # Avoid division by zero\n",
        "      df_normalised[columns_set_17] = \\\n",
        "       (df[columns_set_17].div(row_sums_17, axis=0)) * 100\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error normalizing dataframe from file {file_name}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    return df_normalised"
      ],
      "metadata": {
        "id": "Lnybj7MFFdVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_single_df(df, file_prefix, file_name):\n",
        "    \"\"\"\n",
        "    Process an input `pandas` dataframe to:\n",
        "    - strip leading spaces and replace \"-\" with \" to \" in Age Group columns,\n",
        "    - standardise hyphenation in the Area Name column,\n",
        "    - remove rows where any primary key columns are empty,\n",
        "    - perform meta-data checks, which consist of:\n",
        "          a) Checking for primary key uniqueness\n",
        "          b) Data type match\n",
        "          c) Checking if columns expected to have 1:1 mappings do\n",
        "          d) Checking levels of categorical variables (if any)\n",
        "\n",
        "    In special cases, the following processing steps will additionally be run\n",
        "    before meta-data checks:\n",
        "          a) If \"H-01\" or \"HH-01-Total\", delete all rows where \"Tehsil Code\" <>\n",
        "          \"00000\" or \"Town Code\" <> \"000000\".\n",
        "          b) If \"HL-14-Total\" or \"HL-14-SC-ST\", delete all rows where\n",
        "          \"Tehsil Code\" <> \"00000\" or \"Town Code/Village code\" <> \"000000\" or\n",
        "          \"Ward No <> \"0000\", and ensure values across proportion columns sum up\n",
        "          to 100 in each row.\n",
        "          c) If \"F-04\", \"F-08\" or \"F-12\", remove leading whitespaces from \"Economic Activity\"\n",
        "          column.\n",
        "\n",
        "    Failure to satisfy any meta-data check will result in an error.\n",
        "\n",
        "    Parameters:\n",
        "    - df (str): Input `pandas` dataframe.\n",
        "    - file_prefix (str): Excel file name prefix.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame: Processed DataFrame.\n",
        "    \"\"\"\n",
        "    # Replace \"-\" with \" to \" in column with names \"Age Group\", \"Age-Group\" etc.\n",
        "    age_col_names = [\"Age Group\", \"Age group\", \"Age-Group\", \"Age-group\",\n",
        "                     \"Age_Group\", \"Age_group\"]\n",
        "    age_cols = [col for col in df.columns if col in age_col_names]\n",
        "    df[age_cols] = df[age_cols].apply(\n",
        "        lambda x: x.astype(str).str.replace(\"-\", \" to \").str.lstrip())\n",
        "\n",
        "    # Standardise Hyphenation in the Area Name column\n",
        "    df['Area Name'] = df['Area Name'].str.replace(\n",
        "        r'(?<=\\S)-(?=\\S)', ' - ', regex=True)\n",
        "    df['Area Name'] = df['Area Name'].str.replace(\n",
        "        r'(?<=\\S)- (?=\\S)', ' - ', regex=True)\n",
        "    df['Area Name'] = df['Area Name'].str.replace(\n",
        "        r'(?<=\\S) -(?=\\S)', ' - ', regex=True)\n",
        "\n",
        "    metadata = metadata_cache[file_prefix]\n",
        "    # --- Step 1: Special Case Processing ---\n",
        "    if file_prefix in [\"H-01\", \"HH-01-Total\"]:\n",
        "        df = df[(df[\"Tehsil Code\"] == \"00000\") & (df[\"Town Code\"] == \"000000\")]\n",
        "\n",
        "    if file_prefix in [\"HL-14-Total\", \"HL-14-SC-ST\"]:\n",
        "        df = df[\n",
        "            (df[\"Tehsil Code\"] == \"00000\") &\n",
        "            (df[\"Town Code/Village code\"] == \"000000\") &\n",
        "            (df[\"Ward No\"] == \"0000\")\n",
        "        ]\n",
        "\n",
        "        # Ensure proportion columns sum to 100\n",
        "        df = normalise_dataframe(df, file_name)\n",
        "\n",
        "    if file_prefix in [\"F-04\", \"F-08\", \"F-12\"]:\n",
        "        df[\"Economic Activity\"] = df[\"Economic Activity\"].str.lstrip()\n",
        "\n",
        "    # --- Step 2: Remove rows where any primary key columns are empty ---\n",
        "    primary_keys = [col for col, meta in metadata.items()\n",
        "                    if meta.get(\"Primary Key\")]\n",
        "    df = df.dropna(subset=primary_keys)\n",
        "\n",
        "    # --- Step 3: Metadata Checks ---\n",
        "    # (a) Primary Key Uniqueness\n",
        "    if df.duplicated(subset=primary_keys).any():\n",
        "        raise ValueError(f\"File {file_name}: Primary key constraint violated.\")\n",
        "\n",
        "    # Loop for Data Type Check, 1:1 Mapping Check, Categorical Levels Check\n",
        "    map_1to1_groups = {}  # Dict to store columns grouped by mapping symbol\n",
        "\n",
        "    for col, meta in metadata.items():\n",
        "        expected_type = meta.get(\"Type\")\n",
        "        expected_levels = meta.get(\"Levels\")\n",
        "        mapping_symbol = meta.get(\"Map 1:1\")\n",
        "\n",
        "        # --- (b) Data Type Validation ---\n",
        "        if expected_type == \"numerical\":\n",
        "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "                raise ValueError((f\"File {file_name}: Column '{col}' expected \"\n",
        "                            f\"to be numerical, but found non-numeric values.\"))\n",
        "\n",
        "        elif expected_type == \"constant string\":\n",
        "            actual_levels = df[col].dropna().unique()\n",
        "            if set(actual_levels) != set(expected_levels):\n",
        "                raise ValueError((f\"File {file_name}: Column '{col}' expected \"\n",
        "                  f\"to have level{expected_levels}, but found {actual_levels}.\"))\n",
        "\n",
        "        elif expected_type == \"constant string within\":\n",
        "            if df[col].nunique() > 1:\n",
        "                raise ValueError((f\"File {file_name}: Column '{col}' expected \"\n",
        "                                  f\"to have the same value across all rows.\"))\n",
        "\n",
        "        # --- (c) Collect 1:1 Mapping Groups ---\n",
        "        if mapping_symbol:\n",
        "            if mapping_symbol in map_1to1_groups:\n",
        "                map_1to1_groups[mapping_symbol].append(col)\n",
        "            else:\n",
        "                map_1to1_groups[mapping_symbol] = [col]\n",
        "\n",
        "        # --- (d) Categorical Levels Check ---\n",
        "        if expected_levels and expected_type not in [\"constant string\",\n",
        "                                                     \"constant string within\"]:\n",
        "            actual_levels = df[col].dropna().unique()\n",
        "            unexpected_levels = set(actual_levels) - set(expected_levels)\n",
        "            if unexpected_levels:\n",
        "                raise ValueError((f\"File {file_name}: Unexpected levels found \"\n",
        "                                  f\"in column '{col}':{unexpected_levels}\"))\n",
        "\n",
        "    # --- Step 3(c) Validate 1:1 Mappings ---\n",
        "    for mapping_symbol, columns in map_1to1_groups.items():\n",
        "        if len(columns) != 2:\n",
        "            raise ValueError((f\"File {file_name}: Mapping symbol '{mapping_symbol}' \"\n",
        "              f\"should be assigned to exactly two columns, but found {columns}.\"))\n",
        "\n",
        "        col1, col2 = columns\n",
        "        mapping_pairs = df[[col1, col2]].dropna().drop_duplicates()\n",
        "\n",
        "        if mapping_pairs[col1].duplicated().any() or \\\n",
        "          mapping_pairs[col2].duplicated().any():\n",
        "            raise ValueError((f\"File {file_name}: Columns '{col1}' and '{col2}'\"\n",
        "                              f\" do not have a 1:1 mapping.\"))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "xN8p_Ej5ONhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_process_file(file_info, start_row, file_prefix,\n",
        "                              direct_file_name=None, direct_folder_id=None):\n",
        "    \"\"\"\n",
        "    Downloads and processes a single file.\n",
        "\n",
        "    Parameters:\n",
        "    - file_info (dict): Metadata of the file to download.\n",
        "    - start_row (int): Row in file where the data table starts (1-based index).\n",
        "    - file_prefix (str): Prefix of file names to process.\n",
        "    - direct_file_name (str): Name of the file (with extension, e.g., \".xlsx\") to\n",
        "        be downloaded and processed. If provided, will be directly downloaded\n",
        "        and processed. No search for filename based on `file_prefix` will be done.\n",
        "    - direct_folder_id (str): ID of the Google Drive folder where the \"direct\"\n",
        "        file can be found.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: The processed DataFrame, or None if an error occurred.\n",
        "    \"\"\"\n",
        "    column_names = metadata_cache[file_prefix].keys()\n",
        "    str_cols_dict = {k:'str' for k, v in metadata_cache[file_prefix].items() \\\n",
        "        if v['Type'] in ['string', 'constant string within', 'constant string']}\n",
        "\n",
        "    # Determine file ID\n",
        "    if direct_file_name and direct_folder_id:\n",
        "        try:\n",
        "            query = f\"'{direct_folder_id}' in parents and name = '{direct_file_name}'\"\n",
        "            response = drive_service.files().list(\n",
        "                q=query, spaces='drive', fields=\"files(id)\").execute()\n",
        "            files = response.get(\"files\", [])\n",
        "            if not files:\n",
        "                print(f\"Error finding file {direct_file_name} in given folder.\")\n",
        "                return None\n",
        "            file_id = files[0][\"id\"]\n",
        "        except Exception as e:\n",
        "            print(f\"Error finding file {direct_file_name}: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        file_id = file_info['id']\n",
        "        file_name = file_info['name']\n",
        "\n",
        "    # Download the file\n",
        "    try:\n",
        "        request = drive_service.files().get_media(fileId=file_id)\n",
        "        fh = io.BytesIO()\n",
        "        downloader = MediaIoBaseDownload(fh, request)\n",
        "        done = False\n",
        "        while not done:\n",
        "            status, done = downloader.next_chunk()\n",
        "        fh.seek(0)  # Reset the pointer in BytesIO\n",
        "        df = pd.read_excel(fh, skiprows=start_row - 1, names=column_names,\n",
        "                        dtype=str_cols_dict, usecols = range(len(column_names)))\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading file {direct_file_name or file_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Apply processing logic\n",
        "    processed_df = process_single_df(df, file_prefix, direct_file_name or file_name)\n",
        "    return processed_df"
      ],
      "metadata": {
        "id": "-OoRaijfiAPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to upload single processed dataframe as a spreadsheet in a specified Google Drive folder"
      ],
      "metadata": {
        "id": "d9ANSVxQ1Op_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_as_gsheet(df, op_sheet_name, op_folder_id):\n",
        "    \"\"\"\n",
        "    Uploads input dataframe to a specified Google Drive folder as a Google Sheet\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): Dataframe to Upload\n",
        "    - op_sheet_name (str): Name of the output Google Sheet\n",
        "    - op_folder_id (str): ID of the Google Drive folder where the sheet will be saved\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Save to a temporary CSV\n",
        "        temp_csv = \"temp_combined_data.csv\"\n",
        "        df.to_csv(temp_csv, index=False)\n",
        "\n",
        "        # Prepare file metadata with folder ID\n",
        "        file_metadata = {\n",
        "            \"name\": op_sheet_name,\n",
        "            \"mimeType\": \"application/vnd.google-apps.spreadsheet\",\n",
        "            \"parents\": [op_folder_id]\n",
        "        }\n",
        "\n",
        "        # Use resumable upload by setting the resumable flag to True\n",
        "        media = MediaFileUpload(temp_csv, mimetype=\"text/csv\", resumable=True)\n",
        "        request = drive_service.files().create(body=file_metadata, media_body=media, fields=\"id\")\n",
        "\n",
        "        response = None\n",
        "        while response is None:\n",
        "            status, response = request.next_chunk()\n",
        "            if status:\n",
        "                print(\"Uploaded {}%\".format(int(status.progress() * 100)))\n",
        "        print(\"Data uploaded to Google Sheet: {} , ID: {}\".format(op_sheet_name, response.get(\"id\")))\n",
        "    except Exception as e:\n",
        "        print(\"Error during data upload: {}\".format(e))"
      ],
      "metadata": {
        "id": "8oKWIPbO1bKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to combine (SQL `union` style) a list of pandas dataframes, checking uniqueness of records in combined dataframe and upload the result as a spreadsheet in a specified Google Drive folder\n",
        "\n",
        "- Since each individual dataframe to be combined has already been checked for uniqueness of records, the only reason there could be duplicates in the combined dataframe is if there are duplicate individual dataframes. In this case, an error will be thrown."
      ],
      "metadata": {
        "id": "t2vaLXL6nQuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def union_and_upload_to_gsheet(df_list, op_sheet_name, op_folder_id,\n",
        "                               primary_keys, individual_df_file_name_list):\n",
        "    \"\"\"\n",
        "    Combines DataFrames provided in a list and uploads the result to a specified\n",
        "    Google Drive folder as a single Google Sheet. If, however, the combined\n",
        "    DataFrame exceeds Google Sheets' size limit (10 million cells), each\n",
        "    individual DataFrame is uploaded separately as an individual Google Sheet.\n",
        "\n",
        "    Parameters:\n",
        "    - df_list (List[pd.DataFrame]): List of DataFrames to combine.\n",
        "    - op_sheet_name (str): Name of the output Google Sheet, in case the combined\n",
        "                           dataframe can be uploaded as a single sheet.\n",
        "    - op_folder_id (str): ID of the Google Drive folder where the sheet will be\n",
        "                          saved.\n",
        "    - primary_keys (list): List of names of columns comprising the primary key.\n",
        "    - individual_df_file_name_list(list): List of filenames corresponding to each\n",
        "                                         DataFrame in `df_list` that will be used\n",
        "                                         in case individual DataFrames are to\n",
        "                                         be uploaded separately.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Combine all DataFrames into one\n",
        "        combined_df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "        # Check for duplicates - just to ensure there are no duplicate dfs in themselves\n",
        "        if combined_df.duplicated(subset=primary_keys).any():\n",
        "            raise ValueError(f\"Primary key constraint violated in the combined DataFrame.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during data union: {e}\")\n",
        "\n",
        "    if combined_df.shape[0]*combined_df.shape[1] < 1e7: # 10 million cells limit\n",
        "      upload_as_gsheet(combined_df, op_sheet_name, op_folder_id)\n",
        "    else:\n",
        "      for df_idx, df in enumerate(df_list):\n",
        "        upload_as_gsheet(df, individual_df_file_name_list[df_idx], op_folder_id)"
      ],
      "metadata": {
        "id": "CfJOPCvHh-9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Workflow"
      ],
      "metadata": {
        "id": "_c2tsSza2h6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "vmB0PBhqpEvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate connect to Drive / Sheets\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "drive_service = build('drive', 'v3', credentials=creds)\n",
        "drive.mount('/content/drive')\n",
        "gc = gspread.authorize(creds)"
      ],
      "metadata": {
        "id": "Lsq8OVoIca8n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a603f3c-1e41-41b6-a87e-5ddfdb221ce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set global variables\n",
        "metadata_cache = {}\n",
        "metadata_sheet_id = \"1E3GAnfUaiAhIlU-F2v-9EGlXFymngRmQ-K1pMsawX9s\" # replace with your metadata sheet ID\n",
        "metadata_sheet = gc.open_by_key(metadata_sheet_id)\n",
        "input_folder_path = \"Socratus/Census Data/2011 Data\" # replace with your Google Drive input data folder path\n",
        "input_folder_id = get_folder_id(input_folder_path)\n",
        "output_folder_path = \"Socratus/Gender/Data\" # replace with your Google Drive output folder path\n",
        "output_folder_id = get_folder_id(output_folder_path)"
      ],
      "metadata": {
        "id": "4cajCT_Ki8Mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handle Different Types of Data Files\n",
        "\n",
        "| Filetype / Code | Explanation                                                                                   |\n",
        "|------------------|-----------------------------------------------------------------------------------------------|\n",
        "| Exclude          | Not used for analysis                                                                        |\n",
        "| 1                | Instances - 1 per state - and are available with caste granularity - Total, SC, ST           |\n",
        "| 2                | Singletons                                                                                   |\n",
        "| 3                | Instances available with caste granularity - Total, SC, ST - but are aggregated spatially    |\n",
        "| 4                | Instances - 1 per state - but caste granularity not available                                |\n",
        "| 5                | Instances - 1 per district - but aggregated over caste granularity                          |\n",
        "| 6                | Instances - 1 per state - with caste granularity - SC and ST alone                          |\n"
      ],
      "metadata": {
        "id": "BfhSSWxcjARn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "castes = ['Total', 'SC', 'ST']\n",
        "\n",
        "# File Type 1\n",
        "type1_files_start_with = ['B-01', 'B-03', 'B-07', 'C-02', 'C-20', 'D-02',\n",
        "                          'F-01', 'F-05', 'F-09']\n",
        "type1_files_data_start_row = [9, 8, 9, 8, 6, 6, 11, 11, 11]\n",
        "type1_files_metadata_start_cell = ['F3']*len(type1_files_start_with)\n",
        "\n",
        "# File Type 2\n",
        "type2_files_start_with = ['B-02', 'B-05', 'B-17', 'C-03A', 'C-23']\n",
        "type2_file_names = ['DDW-0000B-02.xlsx', 'DDW-B05-0000.xlsx', 'DDW-B17-0000.xlsx',\n",
        "  'DDW-0000C-03A.xlsx', 'DDW-0000C-23.xlsx']\n",
        "type2_files_data_start_row = [8, 9, 8, 8, 8]\n",
        "type2_files_metadata_start_cell = ['A1']*len(type2_files_start_with)\n",
        "\n",
        "# File Type 3\n",
        "type3_files_start_with = ['B-08', 'HH-04']\n",
        "type3_files_data_start_row = [9, 8]\n",
        "type3_file_names = [['DDW-B08-0000.xlsx', 'DDW-B08SC-0000.xlsx',\n",
        "                    'DDW-B08ST-0000.xlsx'], ['DDW-HH04-0000-2011.XLSx',\n",
        "                    'DDW-HH04SC-0000-2011.XLSx', 'DDW-HH04ST-0000-2011.XLSx']]\n",
        "type3_files_metadata_start_cell = ['A1']*len(type3_files_start_with)\n",
        "\n",
        "# File Type 4\n",
        "type4_files_start_with = ['B-04-Total', 'B-06-Total', 'B-09', 'B-16', 'B-28',\n",
        "  'C-03', 'D-04', 'D-05', 'F-02', 'F-03', 'F-04', 'F-06', 'F-07', 'F-08',\n",
        "  'F-10', 'F-11', 'F-12', 'H-01', 'HH-01-Total', 'HH-02']\n",
        "type4_files_data_start_row = [9, 9, 8, 8, 7, 8, 7, 6, 10, 10, 8, 10, 10, 8, 10,\n",
        "                              10, 8, 8, 7, 7]\n",
        "type4_files_metadata_start_cell = ['D3']*len(type4_files_start_with)\n",
        "\n",
        "# File Type 5\n",
        "type5_files_start_with = ['HL-14-Total']\n",
        "type5_files_data_start_row = [8]\n",
        "type5_files_metadata_start_cell = ['E3']*len(type5_files_start_with)\n",
        "\n",
        "# File Type 6\n",
        "type6_files_start_with = ['B-04-SC-ST', 'B-06-SC-ST', 'HH-01-SC-ST',\n",
        "                          'HL-14-SC-ST']\n",
        "type6_files_data_start_row = [9, 9, 7, 8]\n",
        "type6_files_metadata_start_cell = ['E3']*len(type6_files_start_with)"
      ],
      "metadata": {
        "id": "RIlp0U-4e9Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning Workflow for each Census data dataset:\n",
        "\n",
        "1. Get and Cache Metadata\n",
        "2. Get Dataset Primary Keys (if needed)\n",
        "3. Get Names and IDs of all relevant files (if needed)\n",
        "4. Process the relevant files\n",
        "5. Merge similar datasets (if needed) and output as single google sheet"
      ],
      "metadata": {
        "id": "Z8MbypkN0eqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File Type 1\n",
        "for file_prefix_idx, file_prefix in enumerate(type1_files_start_with):\n",
        "  print(\"---\\n\")\n",
        "  # Get and Cache Metadata\n",
        "  cache_metadata_from_sheet(metadata_sheet, file_prefix,\n",
        "                            type1_files_metadata_start_cell[file_prefix_idx])\n",
        "  # Get Dataset Primary Keys\n",
        "  primary_keys = [col for col, meta in metadata_cache[file_prefix].items()\n",
        "                    if meta.get(\"Primary Key\")]\n",
        "  for caste in castes:\n",
        "    print(\"---\\n\")\n",
        "    # Get Names and IDs of all relevant files\n",
        "    files = get_file_ids_and_names(input_folder_id, file_prefix, caste)\n",
        "    # Process relevant files\n",
        "    processed_dfs = [download_and_process_file(\n",
        "        file, type1_files_data_start_row[file_prefix_idx], file_prefix)\n",
        "        for file in files]\n",
        "    # Output as single google sheet\n",
        "    output_file_name = f\"{file_prefix}_caste_{caste.lower()}\"\n",
        "    individual_df_file_name_list = [\n",
        "        file['name'].rsplit('.', 1)[0] if file['name'].lower().endswith(\n",
        "            ('.xls', '.xlsx')) else file['name'] for file in files]\n",
        "    union_and_upload_to_gsheet(processed_dfs, output_file_name, output_folder_id,\n",
        "                               primary_keys, individual_df_file_name_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCgVP9Bu4BO9",
        "outputId": "3096fcef-9552-4a3a-858a-56674b2b66f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n",
            "Metadata for B-01 cached successfully.\n",
            "---\n",
            "\n",
            "35 files found starting with the prefix 'B-01' (excluding 'B-01_India') and matching caste 'Total'.\n",
            "Data uploaded to Google Sheet: B-01_caste_total , ID: 1a2avF8CvT3bE3ITmz_jOIkW-ox1Ssncfi0nF1BV22qs\n",
            "---\n",
            "\n",
            "31 files found starting with the prefix 'B-01' (excluding 'B-01_India') and matching caste 'SC'.\n",
            "Data uploaded to Google Sheet: B-01_caste_sc , ID: 1rJ0JGpUYHei2VPOG_O4qyWw3zQ0JSRw7K5TT388DurQ\n",
            "---\n",
            "\n",
            "30 files found starting with the prefix 'B-01' (excluding 'B-01_India') and matching caste 'ST'.\n",
            "Data uploaded to Google Sheet: B-01_caste_st , ID: 1Sf9PjgNpR3Ry1TOns1CtlNcdBaZN4pTalGQ0pSyrLgE\n",
            "---\n",
            "\n",
            "Metadata for B-03 cached successfully.\n",
            "---\n",
            "\n",
            "35 files found starting with the prefix 'B-03' (excluding 'B-03_India') and matching caste 'Total'.\n",
            "Data uploaded to Google Sheet: B-03_caste_total , ID: 1cjOjOm-A88nygEKlrE1e90ewzuOjvQCc2EWv6VK3KDY\n",
            "---\n",
            "\n",
            "31 files found starting with the prefix 'B-03' (excluding 'B-03_India') and matching caste 'SC'.\n",
            "Data uploaded to Google Sheet: B-03_caste_sc , ID: 1H47TOJiMQgYHP8bm7gIvMeRSmDXOHckKx0H5x9dzWeU\n",
            "---\n",
            "\n",
            "30 files found starting with the prefix 'B-03' (excluding 'B-03_India') and matching caste 'ST'.\n",
            "Data uploaded to Google Sheet: B-03_caste_st , ID: 1D1QX7E8u-jrCVj7-C9v5PWMwlh0q3CpWNMpWqy9Bp60\n",
            "---\n",
            "\n",
            "Metadata for B-07 cached successfully.\n",
            "---\n",
            "\n",
            "35 files found starting with the prefix 'B-07' (excluding 'B-07_India') and matching caste 'Total'.\n",
            "Data uploaded to Google Sheet: B-07_caste_total , ID: 1jads_N55xzJo_NR6JES3jF_5Li6B28Fn1juvtpxhi4c\n",
            "---\n",
            "\n",
            "31 files found starting with the prefix 'B-07' (excluding 'B-07_India') and matching caste 'SC'.\n",
            "Data uploaded to Google Sheet: B-07_caste_sc , ID: 1Q0SD_Xs65i3wH4G0PXlwnCCt0S_a_Am2Mbhrki0QJ-A\n",
            "---\n",
            "\n",
            "30 files found starting with the prefix 'B-07' (excluding 'B-07_India') and matching caste 'ST'.\n",
            "Data uploaded to Google Sheet: B-07_caste_st , ID: 1CUi9wIB-85uCc57TCjJ9ePxLaqF8j59nEqzVAHEVPN4\n",
            "---\n",
            "\n",
            "Metadata for C-02 cached successfully.\n",
            "---\n",
            "\n",
            "35 files found starting with the prefix 'C-02' (excluding 'C-02_India') and matching caste 'Total'.\n",
            "Data uploaded to Google Sheet: C-02_caste_total , ID: 1xOqfJ4QMgVWnuaCcZlGaXS5ide33rwtbPWX0Fjc_nqM\n",
            "---\n",
            "\n",
            "31 files found starting with the prefix 'C-02' (excluding 'C-02_India') and matching caste 'SC'.\n",
            "Data uploaded to Google Sheet: C-02_caste_sc , ID: 1qxl8Uk1Banf5CYOvj3ucdPJ3zvuouPh02atwqsNzCd8\n",
            "---\n",
            "\n",
            "30 files found starting with the prefix 'C-02' (excluding 'C-02_India') and matching caste 'ST'.\n",
            "Data uploaded to Google Sheet: C-02_caste_st , ID: 1Tgf1Q-XN1J2N9cV-YHtub7H32w7-Ip2X__tviceoHYg\n",
            "---\n",
            "\n",
            "Metadata for C-20 cached successfully.\n",
            "---\n",
            "\n",
            "35 files found starting with the prefix 'C-20' (excluding 'C-20_India') and matching caste 'Total'.\n",
            "Data uploaded to Google Sheet: C-20_caste_total , ID: 15Ejl9ck6cRY27bGyzjGUztCExv4WHd4VlsvJnIrv2ko\n",
            "---\n",
            "\n",
            "31 files found starting with the prefix 'C-20' (excluding 'C-20_India') and matching caste 'SC'.\n",
            "Data uploaded to Google Sheet: C-20_caste_sc , ID: 17ktFLXDht5SlV7o1yopfkrdgHSxOqCQQNPxYmqUPwBs\n",
            "---\n",
            "\n",
            "30 files found starting with the prefix 'C-20' (excluding 'C-20_India') and matching caste 'ST'.\n",
            "Data uploaded to Google Sheet: C-20_caste_st , ID: 1IkGqaNiVfA_hKbUKYmDJu4tp-imrpdU7DTWhz_qbjfA\n",
            "---\n",
            "\n",
            "Metadata for D-02 cached successfully.\n",
            "---\n",
            "\n",
            "35 files found starting with the prefix 'D-02' (excluding 'D-02_India') and matching caste 'Total'.\n",
            "Data uploaded to Google Sheet: D-02_caste_total , ID: 13pc2XGnW_Ll3_klnB6_IjxO1pFXs4vKL1RceO9dhBjk\n",
            "---\n",
            "\n",
            "31 files found starting with the prefix 'D-02' (excluding 'D-02_India') and matching caste 'SC'.\n",
            "Data uploaded to Google Sheet: D-02_caste_sc , ID: 1x1cpumE0nBAtUFTOcr1HYWeIGbjGrEL0A_uynhkwc2U\n",
            "---\n",
            "\n",
            "31 files found starting with the prefix 'D-02' (excluding 'D-02_India') and matching caste 'ST'.\n",
            "Error during data union: Primary key constraint violated.\n",
            "Data uploaded to Google Sheet: D-02_caste_st , ID: 1WhzMkWHNAumWgP80NSA8Ta1rAB75Th4depppQASGfbI\n",
            "---\n",
            "\n",
            "Metadata for F-01 cached successfully.\n",
            "---\n",
            "\n",
            "35 files found starting with the prefix 'F-01' (excluding 'F-01_India') and matching caste 'Total'.\n",
            "Data uploaded to Google Sheet: F-01_caste_total , ID: 1Ieea3Ya0x-zZcmaGuasUrVRRFeNCuHYDX22wIyGg04E\n",
            "---\n",
            "\n",
            "31 files found starting with the prefix 'F-01' (excluding 'F-01_India') and matching caste 'SC'.\n",
            "Data uploaded to Google Sheet: F-01_caste_sc , ID: 1AfQ5DMJTNyZdKuiC6L2Gjo3WjQJb16yoYCCFG2wFuKw\n",
            "---\n",
            "\n",
            "30 files found starting with the prefix 'F-01' (excluding 'F-01_India') and matching caste 'ST'.\n",
            "Data uploaded to Google Sheet: F-01_caste_st , ID: 1EGW2TtuSzQHBbh4B8T9zfGG32_SH8k-D83EwRxUwhbM\n",
            "---\n",
            "\n",
            "Metadata for F-05 cached successfully.\n",
            "---\n",
            "\n",
            "35 files found starting with the prefix 'F-05' (excluding 'F-05_India') and matching caste 'Total'.\n",
            "Data uploaded to Google Sheet: F-05_caste_total , ID: 1vRCKvUD6VrHXDj0fsnleeT_to735LEqxXN9NZZpLzJI\n",
            "---\n",
            "\n",
            "31 files found starting with the prefix 'F-05' (excluding 'F-05_India') and matching caste 'SC'.\n",
            "Data uploaded to Google Sheet: F-05_caste_sc , ID: 1gbkzoA8FEwuObFUQkf8vPIcO0Qdg94eBeAGbDR4kWos\n",
            "---\n",
            "\n",
            "30 files found starting with the prefix 'F-05' (excluding 'F-05_India') and matching caste 'ST'.\n",
            "Data uploaded to Google Sheet: F-05_caste_st , ID: 1DJXUJhSH-xCT4v29JUEJyGLfivBxJ6LuvzmkKTA7gOc\n",
            "---\n",
            "\n",
            "Metadata for F-09 cached successfully.\n",
            "---\n",
            "\n",
            "35 files found starting with the prefix 'F-09' (excluding 'F-09_India') and matching caste 'Total'.\n",
            "Data uploaded to Google Sheet: F-09_caste_total , ID: 1JtWeUkEyuE2F0Jrw89s_yanBIcq6GQ1WRC1a_8vOnTc\n",
            "---\n",
            "\n",
            "31 files found starting with the prefix 'F-09' (excluding 'F-09_India') and matching caste 'SC'.\n",
            "Data uploaded to Google Sheet: F-09_caste_sc , ID: 1jcDxQwHHgFk-IbhtLtZ2q602_NG5JTV4PPm503i-juM\n",
            "---\n",
            "\n",
            "30 files found starting with the prefix 'F-09' (excluding 'F-09_India') and matching caste 'ST'.\n",
            "Data uploaded to Google Sheet: F-09_caste_st , ID: 1Y_lrdgqnwZz1WbpYpLEr7NRaBZzv4tsrsJ9xlh_GdM4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# File Type 2\n",
        "for file_prefix_idx, file_prefix in enumerate(type2_files_start_with):\n",
        "  print(\"---\\n\")\n",
        "  # Get and Cache Metadata\n",
        "  cache_metadata_from_sheet(metadata_sheet, file_prefix,\n",
        "                            type2_files_metadata_start_cell[file_prefix_idx])\n",
        "  # Process relevant files\n",
        "  processed_df = download_and_process_file(\n",
        "      None, type2_files_data_start_row[file_prefix_idx], file_prefix,\n",
        "      type2_file_names[file_prefix_idx], input_folder_id)\n",
        "  # Output as single google sheet\n",
        "  output_file_name = f\"{file_prefix}_caste_total\"\n",
        "  upload_as_gsheet(processed_df, output_file_name, output_folder_id)"
      ],
      "metadata": {
        "id": "ImtsC4yg472C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b25a092-abb3-4e05-bd23-b6557264e51a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n",
            "Metadata for B-02 cached successfully.\n",
            "Data uploaded to Google Sheet: B-02_caste_total , ID: 1E9N_K9Rswh_G9CUk3QDdmgnX5P0aUBoq33NZaWWixyA\n",
            "---\n",
            "\n",
            "Metadata for B-05 cached successfully.\n",
            "Data uploaded to Google Sheet: B-05_caste_total , ID: 1Vjfo1ixyirwW7UkB0Zuiyr_1FPYqWhJzL-1xfvIJF_s\n",
            "---\n",
            "\n",
            "Metadata for B-17 cached successfully.\n",
            "Data uploaded to Google Sheet: B-17_caste_total , ID: 1jaqKtInQ6MX9Ft9TOAf8w_T9d0SpN8zdHlnDf_kHLfU\n",
            "---\n",
            "\n",
            "Metadata for C-03A cached successfully.\n",
            "Data uploaded to Google Sheet: C-03A_caste_total , ID: 1H8kJPlZFK_7E2atOMdPpbAhFZ-lDJCM44Hlo_z7jvtM\n",
            "---\n",
            "\n",
            "Metadata for C-23 cached successfully.\n",
            "Data uploaded to Google Sheet: C-23_caste_total , ID: 1D_wmQqWD2XzT9JlrEU790Su2QBv7WIhl_uhRXUecZzk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# File Type 3\n",
        "for file_prefix_idx, file_prefix in enumerate(type3_files_start_with):\n",
        "  print(\"---\\n\")\n",
        "  # Get and Cache Metadata\n",
        "  cache_metadata_from_sheet(metadata_sheet, file_prefix,\n",
        "                            type3_files_metadata_start_cell[file_prefix_idx])\n",
        "  for caste_idx, caste in enumerate(castes):\n",
        "    print(\"---\\n\")\n",
        "    # Process relevant files\n",
        "    processed_df = download_and_process_file(\n",
        "        None, type3_files_data_start_row[file_prefix_idx], file_prefix,\n",
        "        type3_file_names[file_prefix_idx][caste_idx], input_folder_id)\n",
        "    # Output as single google sheet\n",
        "    output_file_name = f\"{file_prefix}_caste_{caste}\"\n",
        "    upload_as_gsheet(processed_df, output_file_name, output_folder_id)"
      ],
      "metadata": {
        "id": "k50MSB3y4Fc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10d73bc1-4dd7-4ad1-96fc-38298c99b134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n",
            "Metadata for B-08 cached successfully.\n",
            "---\n",
            "\n",
            "Data uploaded to Google Sheet: B-08_caste_Total , ID: 1fAF8zAzcs_9nYWQmqU8TtP33EwOD3na8KDVT22HSK9Q\n",
            "---\n",
            "\n",
            "Data uploaded to Google Sheet: B-08_caste_SC , ID: 1exnuJ8ZIf77Iea9S6GHW9B_qWo1BHUZ1l4S7fchrL54\n",
            "---\n",
            "\n",
            "Data uploaded to Google Sheet: B-08_caste_ST , ID: 1fc8xqA7UfOTZ4_8F_1YC8pFqIxq41L5jMVabkj-UM0k\n",
            "---\n",
            "\n",
            "Metadata for HH-04 cached successfully.\n",
            "---\n",
            "\n",
            "Data uploaded to Google Sheet: HH-04_caste_Total , ID: 1L1eKdOFmqeVu2eGLWf-7FA5C9hAWp2ZrRz2vY18hC90\n",
            "---\n",
            "\n",
            "Data uploaded to Google Sheet: HH-04_caste_SC , ID: 18_2iiEbV50HWF6WFpBhQ6fFeRBmnNj74jb5JX2cHgtM\n",
            "---\n",
            "\n",
            "Data uploaded to Google Sheet: HH-04_caste_ST , ID: 166xJ-m2ZBbURaYsP8ReFrR4onJRCK-x95DqGXz-Dlsc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# File Type 4\n",
        "for file_prefix_idx, file_prefix in enumerate(type4_files_start_with):\n",
        "  print(\"---\\n\")\n",
        "  # Get and Cache Metadata\n",
        "  cache_metadata_from_sheet(metadata_sheet, file_prefix,\n",
        "                            type4_files_metadata_start_cell[file_prefix_idx])\n",
        "  # Get Dataset Primary Keys\n",
        "  primary_keys = [col for col, meta in metadata_cache[file_prefix].items()\n",
        "                    if meta.get(\"Primary Key\")]\n",
        "  # Get Names and IDs of all relevant files\n",
        "  files = get_file_ids_and_names(input_folder_id, file_prefix)\n",
        "  # Process relevant files\n",
        "  processed_dfs = [download_and_process_file(\n",
        "      file, type4_files_data_start_row[file_prefix_idx], file_prefix)\n",
        "      for file in files]\n",
        "  # Output as single google sheet\n",
        "  output_file_name = f\"{file_prefix}_caste_total\"\n",
        "  individual_df_file_name_list = [\n",
        "    file['name'].rsplit('.', 1)[0] if file['name'].lower().endswith(\n",
        "        ('.xls', '.xlsx')) else file['name'] for file in files]\n",
        "  union_and_upload_to_gsheet(processed_dfs, output_file_name, output_folder_id,\n",
        "                              primary_keys, individual_df_file_name_list)"
      ],
      "metadata": {
        "id": "4Xw3_Zxh4IHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afa5fac9-31e8-4648-ef00-8b4cd8d2ee92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n",
            "Metadata for HH-01-Total cached successfully.\n",
            "35 files found starting with the prefix 'HH-01-Total' (excluding 'HH-01-Total_India').\n",
            "Data uploaded to Google Sheet: HH-01-Total_caste_total , ID: 1rKThtCVfugQJO1SwUMu-O3msllGNrWsOHRvwNfu4D6w\n",
            "---\n",
            "\n",
            "Metadata for HH-02 cached successfully.\n",
            "34 files found starting with the prefix 'HH-02' (excluding 'HH-02_India').\n",
            "Data uploaded to Google Sheet: HH-02_caste_total , ID: 1KID9TFXcE3Uw9icjhaigFw7JddgPXCF2_2Gr9mm-6n8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# File Type 5\n",
        "for file_prefix_idx, file_prefix in enumerate(type5_files_start_with):\n",
        "  print(\"---\\n\")\n",
        "  # Get and Cache Metadata\n",
        "  cache_metadata_from_sheet(metadata_sheet, file_prefix,\n",
        "                            type5_files_metadata_start_cell[file_prefix_idx])\n",
        "  # Get Dataset Primary Keys\n",
        "  primary_keys = [col for col, meta in metadata_cache[file_prefix].items()\n",
        "                    if meta.get(\"Primary Key\")]\n",
        "  # Get Names and IDs of all relevant files\n",
        "  files = get_file_ids_and_names(input_folder_id, file_prefix)\n",
        "  # Process relevant files\n",
        "  processed_dfs = [download_and_process_file(\n",
        "      file, type5_files_data_start_row[file_prefix_idx], file_prefix)\n",
        "      for file in files]\n",
        "  # Output as single google sheet\n",
        "  output_file_name = f\"{file_prefix}_caste_total\"\n",
        "  individual_df_file_name_list = [\n",
        "    file['name'].rsplit('.', 1)[0] if file['name'].lower().endswith(\n",
        "        ('.xls', '.xlsx')) else file['name'] for file in files]\n",
        "  union_and_upload_to_gsheet(processed_dfs, output_file_name, output_folder_id,\n",
        "                              primary_keys, individual_df_file_name_list)"
      ],
      "metadata": {
        "id": "8R70bZ2d4HU1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2377ba1-6cdb-498f-8004-197644bc0e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n",
            "Metadata for HL-14-Total cached successfully.\n",
            "640 files found starting with the prefix 'HL-14-Total' (excluding 'HL-14-Total_India').\n",
            "Data uploaded to Google Sheet: HL-14-Total_caste_total , ID: 1JR6NvAuI0fz3T7gjzurdGTnN0vmJTnfUCZ8LiY2hrro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# File Type 6\n",
        "for file_prefix_idx, file_prefix in enumerate(type6_files_start_with):\n",
        "  print(\"---\\n\")\n",
        "  # Get and Cache Metadata\n",
        "  cache_metadata_from_sheet(metadata_sheet, file_prefix,\n",
        "                            type6_files_metadata_start_cell[file_prefix_idx])\n",
        "  # Get Dataset Primary Keys\n",
        "  primary_keys = [col for col, meta in metadata_cache[file_prefix].items()\n",
        "                    if meta.get(\"Primary Key\")]\n",
        "  for caste in [\"sc\", \"st\"]:\n",
        "    print(\"---\\n\")\n",
        "    # Get Names and IDs of all relevant files\n",
        "    files = get_file_ids_and_names(input_folder_id, file_prefix, caste)\n",
        "    # Process relevant files\n",
        "    processed_dfs = [download_and_process_file(\n",
        "        file, type6_files_data_start_row[file_prefix_idx], file_prefix)\n",
        "        for file in files]\n",
        "    # Output as single google sheet\n",
        "    output_file_name = f\"{file_prefix}_caste_{caste.lower()}\"\n",
        "    individual_df_file_name_list = [\n",
        "      file['name'].rsplit('.', 1)[0] if file['name'].lower().endswith(\n",
        "          ('.xls', '.xlsx')) else file['name'] for file in files]\n",
        "    union_and_upload_to_gsheet(processed_dfs, output_file_name, output_folder_id,\n",
        "                                primary_keys, individual_df_file_name_list)"
      ],
      "metadata": {
        "id": "bvwMA_Vy4Mpf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ebceb72-8499-4864-9598-ac5608ff0148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n",
            "Metadata for B-04-SC-ST cached successfully.\n",
            "---\n",
            "\n",
            "31 files found starting with the prefix 'B-04-SC-ST' (excluding 'B-04-SC-ST_India') and matching caste 'sc'.\n",
            "Data uploaded to Google Sheet: B-04-SC-ST_caste_sc , ID: 1eApyTG_-IPIpm8enF2CGfojv5W_VEA_ORoyeYV_27_Q\n",
            "---\n",
            "\n",
            "30 files found starting with the prefix 'B-04-SC-ST' (excluding 'B-04-SC-ST_India') and matching caste 'st'.\n",
            "Data uploaded to Google Sheet: B-04-SC-ST_caste_st , ID: 1jrtXTohr9T8SE_pAjX4iIh3H8Xm6qhWTpe73aHz9cXw\n",
            "---\n",
            "\n",
            "Metadata for B-06-SC-ST cached successfully.\n",
            "---\n",
            "\n",
            "31 files found starting with the prefix 'B-06-SC-ST' (excluding 'B-06-SC-ST_India') and matching caste 'sc'.\n",
            "Data uploaded to Google Sheet: B-06-SC-ST_caste_sc , ID: 1Hy7oFSBFDd96-5F3Yn0UeWGPZ-mRIosPdoqLZucVDso\n",
            "---\n",
            "\n",
            "30 files found starting with the prefix 'B-06-SC-ST' (excluding 'B-06-SC-ST_India') and matching caste 'st'.\n",
            "Data uploaded to Google Sheet: B-06-SC-ST_caste_st , ID: 1TZ9Cb39rV_jF7x4R_-L-WsrFbKdPG-si8qsKiHUryFw\n",
            "---\n",
            "\n",
            "Metadata for HH-01-SC-ST cached successfully.\n",
            "---\n",
            "\n",
            "31 files found starting with the prefix 'HH-01-SC-ST' (excluding 'HH-01-SC-ST_India') and matching caste 'sc'.\n",
            "Data uploaded to Google Sheet: HH-01-SC-ST_caste_sc , ID: 1o1Lk-CY3lcndTi1Gwtaq-IqvnAhV5D1Yb56chd1e2q8\n",
            "---\n",
            "\n",
            "30 files found starting with the prefix 'HH-01-SC-ST' (excluding 'HH-01-SC-ST_India') and matching caste 'st'.\n",
            "Data uploaded to Google Sheet: HH-01-SC-ST_caste_st , ID: 1K9OWhnCGPG6S7UCYu5TZn_UV7IYDx7-MucVe-go0Skg\n",
            "---\n",
            "\n",
            "Metadata for HL-14-SC-ST cached successfully.\n",
            "---\n",
            "\n",
            "31 files found starting with the prefix 'HL-14-SC-ST' (excluding 'HL-14-SC-ST_India') and matching caste 'sc'.\n",
            "Data uploaded to Google Sheet: HL-14-SC-ST_caste_sc , ID: 1JDEOJzFZWQvJ_CAnsMVNDPH7OZyeJ385oDGIeHmwLCo\n",
            "---\n",
            "\n",
            "30 files found starting with the prefix 'HL-14-SC-ST' (excluding 'HL-14-SC-ST_India') and matching caste 'st'.\n",
            "Data uploaded to Google Sheet: HL-14-SC-ST_caste_st , ID: 1HUoqurMoNOOjel9M_g0LOuj_0nQCujmHaLZ_RLvT624\n"
          ]
        }
      ]
    }
  ]
}